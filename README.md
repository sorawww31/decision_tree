<!-- where: README.md / what: project overview and decision tree details / why: help readers understand the implementation -->
# Numpyだけで機械学習
機械学習アルゴリズムを理解するため、Geminiの**ガイド付き学習**を活用して簡単に実装し、理解する

## 決定木

### プロジェクト概要
PythonとNumPyのみを使用して、回帰タスク用の決定木（Regression Tree）をスクラッチで実装。Scikit-learnの実装と比較し、アルゴリズムの挙動と精度の検証を行った。

### 実装の全体像（学習 → 予測）
1. `SimpleDecisionTree.fit` で `X, y` を NumPy 配列に変換し、`_grow_tree` を呼び出す
2. `_grow_tree` が分割点を探しながら再帰的に木を成長させる
3. `predict` はサンプルごとに `_predict_tree` で木を下り、葉ノードの値を返す

### 分割探索のロジック（core.py）
**分割候補**
* 各特徴量 `feat` について、`X[:, feat]` のユニーク値をしきい値候補として総当たりで評価
* `split_data` は `X[:, feat] < threshold` を左、`>=` を右として分割

**評価指標**
* `calculate_variance` で左右それぞれの分散を算出
* `calculate_cost` は左右の分散の**重み付き平均**（サンプル数で重み付け）
* `find_best_split` が最小の分散となる `(feature_index, threshold)` を選択

**同率最小の扱い**
* `val <= best` のときに候補を `bests` に追加し、`seed` を設定したうえで `random.choice` で1つ選択

### 停止条件と葉ノード
* `depth >= max_depth` または `len(y) < min_samples_split` で分割を止める
* 葉ノードの出力は `y` の平均値（回帰タスクの予測値）

### 予測の流れ
* 決定ノードでは `x[feature_index] < threshold` で左/右に分岐
* 葉ノード（数値）に到達したらその値を返す

### 主要クラスと関数
* `decision_tree/tree.py`:
  * `SimpleDecisionTree.fit` / `predict` / `_grow_tree` / `_predict_tree`
* `decision_tree/core.py`:
  * `find_best_split` / `split_data` / `calculate_cost` / `calculate_variance`

### 実装のポイント

* **再帰的構造:** 木の成長プロセス（`_grow_tree`）に再帰関数を使用。
* **分割基準:** 回帰問題であるため、**分散（Variance）の減少**を最大化する分割点（特徴量と閾値）を採用。
* **停止条件:** 過学習を防ぐため、`max_depth`（木の深さ）と `min_samples_split`（分割に必要な最小データ数）を導入。

**主な学び**

1. **過学習（Overfitting）の確認:**
* `max_depth` を大きく（例: 10）すると、訓練データの誤差は減るが、検証データの誤差は増えることを確認。
* 適切な深さ（例: 3）で止めることが汎化性能に重要である。


2. **Scikit-learnとの比較:**
* 自作モデルとScikit-learnの `DecisionTreeRegressor` で、訓練スコアが完全に一致。ロジックの正しさが証明された。
* 検証スコアの微差は、最適な分割候補が複数ある場合の「タイブレーク（同点時の処理）」の違いによるもの（自作は決定的、sklearnは確率的要素あり）。

## 🌲 自作ランダムフォレスト・プロジェクト

### 1. ランダムフォレストの特性とメリット 💡

単一の決定木にはない、このアルゴリズムの強みをまとめます。

* **アンサンブル学習（群衆の知恵）**: 1本1本は「弱い学習器（Weak Learner）」であっても、多数決や平均をとることで、個々の木のミスを打ち消し合い、全体として非常に強力な予測性能を発揮します。
* **バギング（Bootstrap Aggregating）**: データの「行（サンプル）」を重複を許してランダムに抽出することで、多様な木を育てます。
* **特徴量のランダム選択**: ノードごとに使う「列（特徴量）」を制限し、特定の強い特徴量に依存しすぎるのを防ぎます。
* **解釈性（Feature Importance）**: ブラックボックスになりがちな機械学習において、「どの特徴量が分散の減少に最も寄与したか」を数値化して説明できます。

---

### 2. 実装における「つまずき」と解決のプロセス 🛠️

このプロジェクトを通じて直面した技術的な壁と、その乗り越え方です。

#### ❌ つまずき1：特徴量サンプリングのタイミング

* **当初の実装**: 木を作る「前」に特徴量を固定して削っていた。
* **問題点**: 木の成長過程で、本来使えるはずの特徴量に一切アクセスできなくなり、表現力が極端に落ちてしまった。
* **解決**: 標準的な手法に従い、**「ノードの分割（Split）のたび」**にランダムにサンプリングし直すように変更。これにより、木全体としては全ての特徴量を利用できるチャンスを確保した。

#### ❌ つまずき2：乱数生成器（Seed）の状態保持

* **当初の実装**: `find_best_split` の中で毎回 `seed` を指定して `Random` オブジェクトを生成していた。
* **問題点**: 毎回「最初のくじ」を引き直すことになり、どのノードでも全く同じ特徴量が選ばれる「決定論的な」動きになってしまった。
* **解決**: `SimpleDecisionTree` の初期化時に一度だけ生成した `rng` オブジェクトを、**バケツリレーのように再帰関数（`_grow_tree`）へ渡す**設計に変更。これにより、状態が更新され続け、真にランダムな選択が実現した。

#### ❌ つまずき3：重要度（Gain）の計算ロジック

* **当初の実装**: 分割後のコスト（分散）をそのまま足し合わせていた。
* **問題点**: 分散を「減らせなかった（＝予測に貢献しなかった）」特徴量ほど点数が高くなるという逆の結果になった。
* **解決**: **「分割前の分散 - 分割後の分散」** という引き算により、**「どれだけ不純度を減らしたか（利得）」**を計算。これを累積することで、正しい特徴量重要度を算出できた。

---

### 3. ハイパーパラメータの実験結果 🔬

モデルの複雑さと精度の関係を学びました。

* **深さ10**: 学習データには完璧に適合したが、テストデータとの乖離が大きく「過学習（Overfitting）」の状態。
* **深さ3**: 汎化性能は高いが、全体的な精度が伸び悩む「学習不足（Underfitting）」の傾向。
* **結論**: 深さを制御することで、モデルの複雑さを調整し、未知のデータに対する強さをコントロールできることを確認。
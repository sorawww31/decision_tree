# Numpyだけで機械学習
機械学習アルゴリズムを理解するため、Geminiの**ガイド付き学習**を活用して簡単に実装し、理解する

## 決定木

**プロジェクト概要**
PythonとNumPyのみを使用して、回帰タスク用の決定木（Regression Tree）をスクラッチで実装。Scikit-learnの実装と比較し、アルゴリズムの挙動と精度の検証を行った。

**実装のポイント**

* **再帰的構造:** 木の成長プロセス（`_grow_tree`）に再帰関数を使用。
* **分割基準:** 回帰問題であるため、**分散（Variance）の減少**を最大化する分割点（特徴量と閾値）を採用。
* **停止条件:** 過学習を防ぐため、`max_depth`（木の深さ）と `min_samples_split`（分割に必要な最小データ数）を導入。

**主な学び**

1. **過学習（Overfitting）の確認:**
* `max_depth` を大きく（例: 10）すると、訓練データの誤差は減るが、検証データの誤差は増えることを確認。
* 適切な深さ（例: 3）で止めることが汎化性能に重要である。


2. **Scikit-learnとの比較:**
* 自作モデルとScikit-learnの `DecisionTreeRegressor` で、訓練スコアが完全に一致。ロジックの正しさが証明された。
* 検証スコアの微差は、最適な分割候補が複数ある場合の「タイブレーク（同点時の処理）」の違いによるもの（自作は決定的、sklearnは確率的要素あり）。